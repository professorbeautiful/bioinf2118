---
title: "Supplement on the Wilcoxon two-sample test: exact vs approximate, & the effect of ties
"
author: "Roger Day"
date: "3/13/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
---
Supplement on the Wilcoxon two-sample test: exact vs approximate, & the effect of ties
---


```{r include=FALSE}
sampleSize = 100
nFeatures = 300
DIGITS = 3
```


When the data contain ties, the exact distribution of the statistic is much more difficult and the approximations are not good. This doesnâ€™t matter much for P values > 0.001, but can be a huge effect for very very small P values. This matters a lot, because when there are many many comparisons (high-throughput biological data), small P values are frequent, including small P values generated just by chance.

In this numerical experiment, we create a data set with `r sampleSize` observations and `r nFeatures` features, and one binary label per observation.
The effects range widely from zero to 5 standard deviations.
We will see what the effect of ties might be.



```{r}
outcomeClass = rbinom(sampleSize, 1, 1/2)
effectSizes = rnorm(nFeatures) * 2
```
```{r warning=FALSE}
do.wilcoxes = function(Xrounded) {
  X0 <- Xrounded[outcomeClass==0]
  X1 <- Xrounded[outcomeClass==1]
  data.frame(p.approx = wilcox.test(X0, X1)$p.value,
             p.correct = wilcox.test(X0, X1, correct=T)$p.value,
             p.exact = wilcox.test(X0, X1, exact=T)$p.value,
             anyTied = length(unique(Xrounded)) < sampleSize 
                  # not all unique.
   )
}
#options.saved = options(warn=-1)
p.values.list = lapply(effectSizes, function(effSize){
  X = rnorm(sampleSize) + effSize*outcomeClass
  Xrounded <<- round(X, digits=DIGITS)
  do.wilcoxes(Xrounded)
 }
)
#options(options.saved)
```
We suppressed the warning messages, using the chunk option **{r warning=FALSE}**. Warnings are so bourgeois.

Now we combine the results.
```{r}
library(plyr)
p.values.df = ldply(p.values.list)
table(p.values.df$anyTied)

```



Note: plyr is a good library to know! In "ldply", the "l" means the input is a list, and the "d" means the output is a data frame.

### Figure 1
```{r,fig.height=4.5,fig.width=4.5}
colors = c("red","darkgreen")[p.values.df$anyTied+1]  
cex = c(1,1)[p.values.df$anyTied+1]  
pch = c("O", "T")[p.values.df$anyTied+1]  
with(p.values.df, {
    plot(p.approx, p.correct, log="xy", col=colors, pch=pch, cex=cex, 
         xlab="P value: normal approximation", 
         ylab="P value: \"corrected\" approximation")
    abline(a=0,b=1)
    title('"correct=TRUE" (continuity correction)')
  legend(x = 'topleft', legend=c('dataset with ties', 'dataset with NO ties'), pch=c('T', 'O'), col=c('darkgreen', 'red'))
}

)
```


In Figure 1, each point is a data set, with the effect size chosen randomly.
The "green T" is a data set that has at least one tie.
The "red O" has no ties.
The "correct" and "approx" values are virtually identical for a sample size of `r sampleSize`. Note that "correct" does not mean what it claims. 
It means only that a "continuity correction" for the normal approximation was applied, to adjust (roughly) for the discreteness of the Wilcoxon test statistic.  Sounds good; but makes no difference.

## The Exact-o knife


Let's instead compare the approximation to the exact P value.



### Figure 2

```{r}
with(p.values.df, {
    plot(p.approx,p.exact, log="xy", col=colors, pch=pch, cex=cex,
         xlab="P value: normal approximation", 
         ylab="P value: \"exact\" (sometimes)")
    abline(a=0,b=1)
    title('"exact=TRUE" ')
    legend(x = 'topleft', legend=c('dataset with ties', 
                                   'dataset with NO ties'),
           pch=c('T', 'O'), col=c('darkgreen', 'red'))

  }
)
```
Now this is strange!   For data sets with ties, the green T's agree perfectly like before.  But for those with no ties, when the exact P values is really small, the approximation is terrible, hugely bigger, less significant, than it should be.

This matters *a lot*, because when there are many many comparisons (high-throughput biological data), small P values are frequent, including small P values generated just by chance. People apply very stringent P value cutoffs to reduce the risk of false positive "discoveries".

What's going on??

# The effect of ties

When the data contain ties, the exact distribution of the statistic is much MUCH more computationally intensive, and wilcox.text simply will not do it. Instead it just puts out the approximation. (There is a warning message, but as noted above we have suppressed it.) 


Let's look at this by effect size.
For the approximate P values ties and no-ties data sets follow the same shape.

### Figure 4

```{r}
plot(effectSizes, p.values.df$p.approx, pch = pch, cex=1/2, col = colors, log='y')
legend(x = 'topright', legend=c('ties', 'no ties'), pch=c('T', 'O'), col=c('darkgreen', 'red'))
```

But for the "exact" computation, the presence of ties prevent the "exact" method from being used.

### Figure 5


```{r}
plot(effectSizes, p.values.df$p.exact, pch = pch, cex=1/2, col = colors, log='y')
legend(x = 'topright', legend=c('ties', 'no ties'), pch=c('T', 'O'), col=c('darkgreen', 'red'))

```


### Studying one data set.

```{r}
effSize = 2

```

To illustrate, we create a data set with a pretty strong effect size =  `r effSize`. We do a no-tie version, but where the second value is replaced by X[2] = X[1] + 0.0001, very close to a tie.


```{r}
X = rnorm(sampleSize) + effSize*outcomeClass
X[2] = X[1] + 0.0001   #almost a tie
do.wilcoxes(X)
```

The we successively round the data to fewer and fewer digits.

```{r warning=FALSE}
t(
  sapply(6:0, function(digits) {
    c(digits=digits, do.wilcoxes(round(X, digits=digits)))
    }
    )
)
```
Look at the "p.exact" column.
Once you round to 3 or fewer digits, ties appear. And all hell breaks loose.  In the sense that the "exact" calculation is no longer done, but the approximation substitutes for it. 

Is that good?  Well, if you don't mind increasing the P value by two orders of magnitude...   But if you are committed to coping with multiple testing with some "significance" adjustment like the Bonferroni method, then that is a disaster.

I have a long rap about the perils of the P value, and another long rap about the perils of multiple testing, so ... calm down.


## Breaking the ties

Now we will take the digits=3 dataset, but break the ties, by adding a tiny increment to a duplicated value. That should give a more appropriately small P value.
```{r}
Xrounded3 = round(X, digits=3)
do.wilcoxes(Xrounded3)
which(duplicated(Xrounded3))
ties = which(duplicated(Xrounded3))
Xbrokentie = Xrounded3
Xbrokentie[ties] = Xbrokentie[ties] + 1e-7
do.wilcoxes(Xbrokentie)
```
Aha!  Now the exact P value is several orders of magnitude smaller, as it should be.
So we have regained the power lost when the ability to do the exact permutation was lost.

## Computing time
The computing time increases dramatically due to *exact=TRUE*, when there are no ties. *If there are ties, the option is ignored*. We suppress the warnings again.
```{r warning=FALSE}
system.time(for(i in 1:20)do.wilcoxes(Xrounded3))
system.time(for(i in 1:20)do.wilcoxes(Xbrokentie))
```
