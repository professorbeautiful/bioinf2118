---
title: "CondProb new script"
author: "Roger Day"
date: "2025-05-13"
output:
  html_document:
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

$$
\newcommand{\alldata}{\space 
  \color{red}{\mathrm{{all{\space}data}} \space }}
\newcommand{\OR}{\space \mathrm{or}\space}
\newcommand{\AND}{\space \mathrm{and}\space}
\newcommand{\RR}{\color{red}{R1 \AND R2}}
$$

```{css}
table {
  margin: auto;
  border-top: 1px solid #666;
  border-bottom: 1px solid #666;
}
table thead th { border-bottom: 1px solid #ddd; }
th, td { padding: 10px; }
```


##  ##Setup

```{r defaults}

CondProb_folder  = '"/Users/Roger/Sites/Bioinf-2118-2018-website/N-files-for-handouts/CondProb video"'

options(digits=4)
priorFor4 = c(0.9,0.10, 9, 90)/100

names(priorFor4) = c('dD', 'hD',	'iD',	'noD')
longgroupnames = c(dD='detectable', hD='hidden',	
                   iD='imposter',	noD='no disease')
pr_positive_given_group = c(90,10,50, 10)/100
names(pr_positive_given_group) = 
  c('dD', 'hD',	'iD',	'noD')
pr_dist_given_group = rbind(pr_positive_given_group, 1 - pr_positive_given_group) 
rownames(pr_dist_given_group) = c('pos', 'neg')
normalize= function(x)  x/sum(x)

source('makePosteriorSequence.R')
```
## ##Intro

Welcome to Statistics Beautiful.
Today: a pet turtle in the clutches of a child, a prisoner's picnic gone bad, alarming claims like there's visitors from outer space, or "yadayada Hillary Clinton yadayada"....or, birds aren't real!
& how to combine repeated medical tests. All to understand better-- How do we combine information from different sources, why does conditional independence matter, and what can go wrong? And did I mention machine learning? So, strike up the band for the Bayesian dance! 



There's what you know. We'll call that data.   

And there is something you want to know. 

We'll call that the unknown truth, the true state... is the answer YES or NO?, is the statement TRUE or FALSE?, is the SICKNESS PRESENT or ABSENT?, will it be SUNNY or RAINY?.  

In episode 1, we learned how you start with some belief about the true state. A belief expressed as probabilities. Or even better, as *odds*.

Then you use new data to refine that initial belief.

Well odds is just a different way of describing a probability P. Instead of P, think about P over one minus P. Same information, in fact P equals odds over one plus odds. A probability is between zero and one, and an odds is between zero and infinity.
Refresh yourself at this link here if you want. Nine to one odds means zero point 9 divided by zero point 1, nine tenth to one tenth, and nine over 1 plus nine is, sure enough, 9 tenths. 

An alarming possibility, like visitors from outer space, or "yadayada Hillary clinton yadayada"...., 
or "birds aren't real".

They tell me, "Do your own research.

Suppose I start with a very LOW odds that the alarming possibility is true .

I'm after all a reasonable human.

Now I get deluged by a flurry of pieces of evidence supporting the doctrine. Each one pushes up my belief, my confidence in the alarming possibility, my personal probability.

## ##Multiply

I'll just multiply the current odds, the "prior odds", by the ratio of the two different explanations of how you could have got the data. It's TRUE.  Or it's BS.
That's called the likelihood ratio for the data:


$$
\begin{aligned}
LR & = Likelihood \space Ratio \\
& =\frac{
Probability \space that \space you'd \space get \space this \space data \space if \space the \space crazy \space idea \space is \space TRUE}
{Probability \space that \space you'd \space get \space this \space data \space if \space the \space crazy \space idea \space is \space BS}
\end{aligned}
$$


where "*BS*" is short for... mmm...  "*NOT true*".

Here's a maybe more serious example. Switch to a medical diagnosis setting.

## ##Medical table

![](table_for_+D_-D.png)

![](/Users/Roger/Sites/Bioinf-2118-2018-website/N-files-for-handouts/CondProb video/app_Picture1.png)

A link to this app is in the Dungeon, also to Episode 1, which walks you through how to use it.
The prior odds is one percent, quite low. This diagnosis seems like a long shot going in the door.  The LR is high, it's 10, so getting a positive is far more likely if the person is Sick, but now multiply the odds by the LR and you've only boosted the odds up from 1 to a hundred to 1 to 10, the probability only increases 9%, which is 1 over 1 plus 10.

Ah, but here comes more information, more data!  What do we do with THAT?

Repeat the test. Suppose you get the same answer every time.  

$R1$ is Positive, $R2$ is positive, $R3$ is  positive, $R4$ is positive. 

Just multiply by the new data's likelihood ratio. Right?  WRONG! Aw, man!


Let's pretend it's ok for now. Multiplying by the same LR again and again.
That's the same as adding the same *log LR* every time, because the logarithm turns multiplying into adding. These little up-arrows are the same length. So again let's start with odds of 1 to a hundred, probability 9%. The first positive test R1 brings the chance up only to 9%.  The second one, R2, jacks it up to a half. R3 brings it to 91%, and R4 to 99%.

Just look at all that evidence mount up! It seems that $+D$, the disease is present, is nearly certain now.

```{r, results='asis'}
### The nice case. Cond Indep, all good.
  print(makePosteriorSequence(
    prob_pos_given_group = c(0.9, 0, 0, 0.09),
    priorFor4 = c(.01,0,0,.99) 
    ))
```

## ## Careful

We gotta be careful, though.
The general principle, the key idea is: always condition on ALL you know, all together. Like this:

$$
\begin{aligned}
Odds(+D|\alldata) & =\frac{Pr(+D|\alldata )}{Pr(-D|\alldata )}\\
& =\frac{Pr(\alldata \AND +D)/Pr(\alldata )}
      {Pr(\alldata \AND -D)/Pr(\alldata )}\\
& = \frac{Pr(\alldata |+D)}{Pr(\alldata |-D)}\ \frac{Pr(+D)}{Pr(-D)}\\
& = LR(\alldata)\times Odds(+D)
\end{aligned}
$$

In this case, just taking the first two reports, that becomes

$$
\begin{aligned}
Odds(+D|\RR) & =\frac{Pr(+D|\RR)}{Pr(-D|\RR)}\\
& =\frac{Pr(\RR \AND +D)\quad/\space Pr(\RR)}
      {Pr(\RR \AND -D)\quad/\space Pr(\RR)}\\
& = \frac{Pr(\RR|+D)}{Pr(\RR|-D)}\ \frac{Pr(+D)}{Pr(-D)}\\
& =LR(\RR)\times Odds(+D)
\end{aligned}
$$

The celebrated 3Blue1Brown hinted that this at the end of an excellent video on Bayesian method.
![](/Users/Roger/Sites/Bioinf-2118-2018-website/N-files-for-handouts/CondProb video/3b1b-bayes-factors.png)
(He has E for event, we have R for report. And pretty colors.) 
The 2nd Bayes factor (what we're calling likelihood ratio) conditions not just on the new data E2 but on the previous data E1 also. 


If we are lucky, R1 and R2 are CONDITIONALLY INDEPENDENT. Getting the right answer for $R1$ doesn't increase or decrease the chance of a right answer for $R2$. And that's good, it means R2 provides new information.
Just keep multiplying by the next *LR*.

$$
\begin{aligned}
LR(\RR) &= 
\frac{Pr(\RR|+D)}{Pr(\RR|-D)}\\
&= \frac{Pr(R1|+D) \quad Pr(R2|+D)}{Pr(R1|-D) \quad Pr(R2|-D)}\\
&= LR(R1)  \quad  \quad \times  \quad  \quad LR(R2)
\end{aligned}
$$
The numerator splits, and the denominator splits. 
But, careful,  this only works if TWO conditional independence claims are true... one for the numerator ($+D$) and one for the denominator ($-D$).

## ## Oh no,

Here is the same probability table,
![](table_for_+D_-D.png)

but something sneaky lurks underneath
A sequence of repeated  tests don't keep jacking up the probability.
After the first test, none of the next 4 deliver any new information.
Why?


 
## ## What's lurking: 4 groups


```{r, results='asis'}
  print(makePosteriorSequence(
    priorFor4 = c(0.009, 0.001, 0.09, 0.90),
    prob_pos_given_group = c(1,0,1,0) , marginalize=TRUE))
```

Here, the positive tests include  GENUINE false positives, because the patient has some OTHER condition, that the test is picking up when we don't want it to.  A misdiagnosis. An imposter. In the lingo, the test is not *specific* enough. "$iD$", means an imposter, when the real deal is "$dD$" for detectable disease.

What's as bad, there can be GENUINE false negatives. Some patients WITH the target disease might have a variant of the disease that, in the lingo, the test is not sensitive enough. *Hidden disease*. "$hD$".
Sometimes called "occult disease", but that sounds too much like being possessed by demons!

Everybody with the disease we want to detect is $+D$, either $dD$ or $hD$.

Everybody who we want to get a negative test for is $-D$, either $noD$ or $iD$.

So $+D$ splits into two types of patients:  true positives $dD$ and false negatives $hD$.
And $-D$ splits into two types of patients: true negatives $noD$ and false positives $iD$.

So, 4 categories instead of 2. 

```{r group names}
print(
  kable(data.frame(short_name=c('dD', 'hD', 'iD', 'noD'),
           long_name = as.vector(c(dD='detectable', hD='hidden',	
                   iD='imposter',	noD='no disease')), 
           test_result=c('true positive', 'false negative',
                         'false positive', 'true negative')
))
)

```

## ## 4 groups distribution

Each category of patients has its own 
*Probability of a Positive Test*.
This is the behind-the-veil full truth. 

![](table_for_dD_hD_iD_noD.png)

We actually don't have prior information for the 4 groups. In fact, we might not know that the sneaky ones, the hiddens and the imposters,  exist. 

The probabilities in the first table showing probabilities for only +D and -D are still correct, but we cannot distinguish between dD and hD, between iD and noD.

The hidden $hD$ and the imposters $iD$ are explanations for false positives and false negatives. Successive false results often share the same cause. Sometimes, two false positives in a row occur because they share the patient, and the patient has an imposter condition that explains both false positives. If you only knew that, you wouldn't be fooled so easily.

Too bad we don't know that.

After time passes, following up on the patients, we may learn which patients really had the disease, so we learn which of the test results had been false.
Sometimes we can do biomedical research, learn that these subgroups exist, and find biomarkers for them.



```{r, asis}
priorFor4 = c(0.9,0.10, 9, 90)/100
names(priorFor4) = c('dD', 'hD',	'iD',	'noD')
pr_positive_given_group = c(90,10, 50, 10)/100
names(pr_positive_given_group) = 
  c('dD', 'hD',	'iD',	'noD')
###  unused?
```

## ## dD and hD = +D

Let's  suppose we have just two test results, $R1 = positive, R2 = positive$. And, that conditional independence IS true for the 4 subgroups. 
For each of the FOUR subgroups of patients, 
$$
\begin{aligned}
Pr(positive, positive \space | \space group \space G) 
        & = Pr(positive \space | \space \space group \space G) {^2} \quad 
\end{aligned}
$$
In particular, for G equals dD and hD. 
Then

$$
\begin{aligned}
Pr(\RR \space | \space +D) & = Pr(\RR \space | \space dD) \quad {Pr(dD | +D)} \\
        & + Pr(\RR \space | \space hD) \quad  {Pr(hD | +D)} \\  
        & = Pr(positive \space | \space dD) {^2} \quad  {0.009/0.01} \\
        & + Pr(positive  \space | \space hD) {^2} \quad  {0.001/0.01} \\
        & = 1.0^2 * 0.009/0.01 + 0.0^2 * 0.001/0.01  \\
        & = 0.9
\end{aligned}
$$
But if the two tests were independent conditionally on $+D$, we'd get


$$
\begin{aligned}
Pr(\RR \space | \space +D) 
        & = Pr(positive \space | \space +D) {^2} \\
        & = 0.9^2 \\
        & = 0.81
\end{aligned}
$$

Nope.  That's wrong.

That the true probability is larger makes sense.  After the first positive test result, only dD and iD are possible. And they both GUARANTEE that the next result will be positive... a true positive for dD and a false positive for iD.

After the 1st report, the next bunch contribute no new information. $Pr(R2 = positive | R1 = positive) = 1$ whether the patient is a true detectable dD or an imposter iD.

Any data whose probability equals one no matter what the truth is, is useless for discovering the truth!   


## ## 2 or more negatives

What if you get all NEGATIVE tests. Maybe the doctor really really really doesn't want to miss it. 

(Actually, it could be that as time passes a hidden disease becomes detectable. That's why every few years we repeat some cancer screening tests.  That's an important case, but we'll set that aside for now.)

```{r, results='asis'}
  print(makePosteriorSequence(dataSequence = c(2,2,2,2),
    priorFor4 = c(0.009, 0.001, 0.09, 0.90),
    prob_pos_given_group = c(1,0,1,0) , marginalize=TRUE))
```
Sure enough, after the first test, the rest are guaranteed to provide no further information. Here is the full story.

```{r, results='asis'}
  print(makePosteriorSequence(dataSequence = c(2,2,2,2),
    priorFor4 = c(0.009, 0.001, 0.09, 0.90),
    prob_pos_given_group = c(1,0,1,0) , marginalize=FALSE))
```

The negatives wipe out the detectable dD's. But not the hidden ones, hD.

Now, suppose there are no imposters.



Ideally you could carve up the population of patients into groups using explanations for why the groups differ. Then the only thing remaining would be pure chance mistakes, not explained by something about the particular patient. Conditional independence. 



And so we return to conspiracy theories and algorithms.

## ## Media bubbles

The algorithm recommends a video that shows that 
![birds are not real!](/Users/Roger/Sites/Bioinf-2118-2018-website/N-files-for-handouts/CondProb video/birds-are-not-real-1.png)
They are all just Hillary Clinton wearing feathers.

Naa, that's silly.  But kind of fun. So you watch the video. The algorithm feeds you another one, another 'proof'. Your posterior probability goes up a bit. Then another, and another.  All these "independent" pieces of evidence.
But are they independent? Really? 
No, they all just replicate a common origin.

So friends, be kind to our neighbors in media bubbles.
They don't know about conditional independence.


## ## Factoring joint distributions

Let's dive in deeper.  (Or, 'delve' in, as ChatGPT would say.)

Suppose you have a bunch of, say, ten random quantities $R1, ... , R10$.
You can ALWAYS write their joint distribution, in a monstrous string.
$$
\color{blue}{Pr(R1)} \times Pr(R2 | R1) \times Pr(R3 | R1, R2)...  \times Pr(R10 | R1, R2, ... R9)
$$
The order is arbitrary.  But you need a starting point, in this case $\color{blue}{Pr(R1)}$ , that is unconditional on anything else.

## ## Causal diagrams and colliders

Complicated! But with conditional independence, way simpler.

$$
Pr(R1) \times Pr(R2 | R1) \times Pr(R3 | R2)...  \times Pr(R10 | R9)
$$
What happens as you go get the next report depends only on the current report Everything before is forgotten.  So $Pr(R10 | R9)$ instead of $Pr(R10 | R1,R2,...,R9)$

This chain of influence is called a Markov chain.
This *pet turtle* decides where to go next after its owner spins it around randomly without picking it up and moving it. It knows where it is, but where it was before is forgotten.


## ## Monte Carlo Markov Chain 

Getting back to the general case,
$$
\color{blue}{Pr(R1)} \times Pr(R2 | R1) \times Pr(R3 | R1, R2)...  \times Pr(R10 | R1, R2, ... R9)
$$

Remember you need a starting point. One of the 10 random reports, you have to know its *marginal* probabilities ignoring all the others, *unconditional* all the others. If you don't know ANY unconditional distribution, do you give up? No! Techniques like Monte Carlo Markov Chain are wildly popular. They contribute to heavy duty data methods like Machine Learning. 

Food for another video.



