---
title: "CondProb new script"
author: "Roger Day"
date: "2025-05-13"
output:
  html_document:
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

$$
\newcommand{\alldata}{\space 
  \color{red}{\mathrm{{all{\space}data}} \space }}
\newcommand{\OR}{\space \mathrm{or}\space}
\newcommand{\AND}{\space \mathrm{and}\space}
\newcommand{\RR}{\color{red}{R1 \AND R2}}
$$

```{css}
table {
  margin: auto;
  border-top: 1px solid #666;
  border-bottom: 1px solid #666;
}
table thead th { border-bottom: 1px solid #ddd; }
th, td { padding: 10px; }
```


##  ##Setup

```{r defaults}

CondProb_folder  = '"/Users/Roger/Sites/Bioinf-2118-2018-website/N-files-for-handouts/CondProb video"'

options(digits=4)
priorFor4 = c(0.9,0.10, 9, 90)/100

names(priorFor4) = c('dD', 'hD',	'iD',	'noD')
longgroupnames = c(dD='detectable', hD='hidden',	
                   iD='imposter',	noD='no disease')
pr_positive_given_group = c(90,10,50, 10)/100
names(pr_positive_given_group) = 
  c('dD', 'hD',	'iD',	'noD')
pr_dist_given_group = rbind(pr_positive_given_group, 1 - pr_positive_given_group) 
rownames(pr_dist_given_group) = c('pos', 'neg')
normalize= function(x)  x/sum(x)

makePosteriorSequence = function(
    priorFor4 = c(0.9,0.10, 9, 90)/100,
    pr_pos_given_group = pr_dist_given_group[1,],
    data = 1,  ### 1's and 2's
  dataSequence = c(1,1,1,1),
  wordsForData = c("'Pos'\\quad", "'Neg'\\quad"),
  dropZeroGroups = TRUE,
  makeHTML = TRUE, 
  marginalize = FALSE) {
  
  pr_dist_given_group = rbind(pr_pos_given_group,  
                              1 - pr_pos_given_group)
  
  names(priorFor4) = c('dD', 'hD',	'iD',	'noD')
  posteriorFor4 = posteriorSequence = priorFor4
  sapply(seq(along=dataSequence), function(ndata) {
    data = dataSequence[ndata]
    posteriorFor4 <<- matrix(normalize(
      posteriorFor4 * pr_dist_given_group[data,]), nrow=1) 
    rownames(posteriorFor4) =  paste0('Report ', ndata, ':', wordsForData[data])
    posteriorSequence <<- rbind(posteriorSequence, posteriorFor4)
  })
  if(dropZeroGroups){
    posteriorSequence = posteriorSequence[ 
      , which (priorFor4>0)]
    rownames(posteriorSequence)[1] = "prior for 2 groups"
  } else {
  #rbind(priorFor4, (posteriorSequence))
    rownames(posteriorSequence)[1] = "prior for 4 groups"
  }
  if(marginalize) {
    Dplus = apply(posteriorSequence[ , 1:2], 1, sum)
    Dminus = apply(posteriorSequence[ , 3:4], 1, sum)
    posteriorSequence[, 1:2] = cbind(Dplus, Dminus)
    posteriorSequence = posteriorSequence[ , 1:2]
  }
  if(ncol(posteriorSequence) == 2){
    colnames(posteriorSequence) = c("+D", "-D")
    rownames(posteriorSequence)[1] = "prior for 2 groups"
  }
  #if( makeHTML)
    return(knitr::kable(format='html',
                  posteriorSequence) )
}
```
## ##Intro

Today: a pet turtle in the clutches of a child, a prisoner's picnic gone bad, a flat earth theory, a political silo, & how to combine medical tests. How do we combine information from different sources, why does conditional independence matter, and what can go wrong? And of course, how does it all relate to machine learning. So, strike up the music for the Bayesian dance! 

There's what you know. We'll call that data.   

And there is something you want to know. 

We'll call that the unknown truth, the true state... is the answer YES or NO?, is the statement TRUE or FALSE?, is the SICKNESS PRESENT or ABSENT?, will it be SUNNY or RAINY?.  

In episode 1, we learned how you start with some belief about the true state. A belief expressed as probabilities. Or even better, as *odds*.

Then you use new data to refine that initial belief.

An alarming possibility, like visitors from outer space, or "yadayada Hillary clinton yadayada"....
They tell me, "Do your own research.

Suppose I start with a very LOW odds that the alarming possibility is true .

I'm after all a reasonable human.

Now I get deluged by a flurry of pieces of evidence supporting the doctrine. Each one pushes up my belief, my confidence in the alarming possibility, my personal probability.

## ##Multiply

I'll just multiply the current odds, the "prior odds", by the ratio of the two different explanations of how you could have got the data. It's TRUE.  Or it's BS.
That's called the likelihood ratio for the data:


$$
\begin{aligned}
LR & = Likelihood \space Ratio \\
& =\frac{
Probability \space that \space you'd \space get \space this \space data \space if \space the \space crazy \space idea \space is \space TRUE}
{Probability \space that \space you'd \space get \space this \space data \space if \space the \space crazy \space idea \space is \space BS}
\end{aligned}
$$


where "*BS*" is short for... mmm...  "*NOT true*".

Here's a maybe more serious example. Switch to a medical diagnosis setting.

## ##Medical table

![](table_for_+D_-D.png)

![](/Users/Roger/Sites/Bioinf-2118-2018-website/N-files-for-handouts/CondProb video/app_Picture1.png)

A link to this app is in the Dungeon, also to Episode 1, which walks you through how to use it.
The prior odds is one percent, quite low. This diagnosis seems like a long shot going in the door.  The LR is high, it's 10, so getting a positive is far more likely if the person is Sick, but now multiply the odds by the LR and you've only boosted the odds up from 1 to a hundred to 1 to 10, the probability only increases 9%, which is 1 over 1 plus 10.

Ah, but here comes more information, more data!  What do we do with THAT?

Repeat the test. Suppose you get the same answer every time.  

$R1$ is Positive, $R2$ is positive, $R3$ is  positive, $R4$ is positive. 

Just multiply by the new data's likelihood ratio. Right?  WRONG! Aw, man!


Let's pretend it's ok for now. Multiplying by the same LR again and again.
That's the same as adding the same *log LR* every time, because the logarithm turns multiplying into adding. These little up-arrows are the same length. So again let's start with odds of 1 to a hundred, probability 9%. The first positive test R1 brings the chance up only to 9%.  The second one, R2, jacks it up to a half. R3 brings it to 91%, and R4 to 99%.

Just look at all that evidence mount up! It seems that $+D$, the disease is present, is nearly certain now.

```{r, results='asis'}
### The nice case. Cond Indep, all good.
  print(makePosteriorSequence(
    pr_pos_given_group = c(0.9, 0, 0, 0.09),
    priorFor4 = c(.01,0,0,.99)))
```

## ## Careful

We gotta be careful, though.
The general principle, the key idea is: always condition on ALL you know, all together. Like this:

$$
\begin{aligned}
Odds(+D|\alldata) & =\frac{Pr(+D|\alldata )}{Pr(-D|\alldata )}\\
& =\frac{Pr(\alldata \AND +D)/Pr(\alldata )}
      {Pr(\alldata \AND -D)/Pr(\alldata )}\\
& = \frac{Pr(\alldata |+D)}{Pr(\alldata |-D)}\ \frac{Pr(+D)}{Pr(-D)}\\
& = LR(\alldata)\times Odds(+D)
\end{aligned}
$$

In this case, just taking the first two reports, that becomes

$$
\begin{aligned}
Odds(+D|\RR) & =\frac{Pr(+D|\RR)}{Pr(-D|\RR)}\\
& =\frac{Pr(\RR \AND +D)\quad/\space Pr(\RR)}
      {Pr(\RR \AND -D)\quad/\space Pr(\RR)}\\
& = \frac{Pr(\RR|+D)}{Pr(\RR|-D)}\ \frac{Pr(+D)}{Pr(-D)}\\
& =LR(\RR)\times Odds(+D)
\end{aligned}
$$

If we are lucky, R1 and R2 are CONDITIONALLY INDEPENDENT. Getting the right answer for $R1$ doesn't increase or decrease the chance of a right answer for $R2$. And that's good, it means R2 provides new information.
Just keep multiplying by the next *LR*.

$$
\begin{aligned}
LR(\RR) &= 
\frac{Pr(\RR|+D)}{Pr(\RR|-D)}\\
&= \frac{Pr(R1|+D) \quad Pr(R2|+D)}{Pr(R1|-D) \quad Pr(R2|-D)}\\
&= LR(R1)  \quad  \quad \times  \quad  \quad LR(R2)
\end{aligned}
$$
The numerator splits, and the denominator splits. 
But, careful,  this only works if TWO conditional independence claims are true... one for the numerator ($+D$) and one for the denominator ($-D$).

Here is the same probability table,
![](table_for_+D_-D.png)

but a way different response to multiple tests, because SOMETHING is lurking underneath:


```{r, results='asis'}
  # print(knitr::kable(makePosteriorSequence(), format='html'))
  # print(knitr::kable(makePosteriorSequence(priorFor4 = c(.01,0,0,.99)), format='html'))
  print(makePosteriorSequence(
    priorFor4 = c(0.009, 0.001, 0.09, 0.90),
    pr_pos_given_group = c(1,0,1,0) , marginalize=TRUE))

```

## ## What's lurking: 4 groups


Here, the positive tests include  GENUINE false positives, because the patient has some OTHER condition, that the test is picking up when we don't want it to.  A misdiagnosis. An imposter. In the lingo, the test is not *specific* enough. "$iD$", means an imposter, when the real deal is "$dD$" for detectable disease.

What's as bad, there can be GENUINE false negatives. Some patients WITH the target disease might have a variant of the disease that, in the lingo, the test is not sensitive enough. Hidden disease. "$hD$".
Everybody with the disease we want to detect is $+D$, either $dD$ or $hD$.
Everybody who we want to get a negative test for is $-D$, either $noD$ or $iD$.

So $+D$ splits into two types of patients:  true positives $dD$ and false negatives $iD$.
And $-D$ splits into two types of patients:  $iD$ and $noD$.

So, 4 categories instead of 2. Each category of patients has its own Probability of a Positive test.


## ## 4 groups distribution

The full story is this:

![](table_for_dD_hD_iD_noD.png)

This is the hidden full truth. We actually don't have prior information for the 4 groups. In fact, we might not know that the sneaky ones, hD and iD,  exist. 

The probabilities in the first table showing probabilities for only +D and -D are still correct, but we cannot distinguish between dD and hD, between iD and noD.

The hidden $hD$ and the imposters $iD$ are explanations for false positives and false negatives.  Too bad we don't know that.

Maybe we can do biomedical research, learn that these groups exist, find biomarkers for them.

Successive false results often share the same cause. In this case, two false positives occur because they share the patient, and the patient has an imposter condition that explains both false positive. If you only knew that, you wouldn't be fooled so easily.

```{r, asis}
priorFor4 = c(0.9,0.10, 9, 90)/100
names(priorFor4) = c('dD', 'hD',	'iD',	'noD')
pr_positive_given_group = c(90,10, 50, 10)/100
names(pr_positive_given_group) = 
  c('dD', 'hD',	'iD',	'noD')
###  unnecessary?
```
## ## dD and hD = +D

Let's  suppose we have just two test results, $R1 = positive, R2 = positive$. And, that conditional independence IS true for the 4 subgroups. 
For each of the FOUR subgroups of patients, 
$$
\begin{aligned}
Pr(positive, positive \space | \space group \space G) 
        & = Pr(positive \space | \space \space group \space G) {^2} \quad 
\end{aligned}
$$
In particular, for G equals dD and hD. 
Then

$$
\begin{aligned}
Pr(\RR \space | \space +D) & = Pr(\RR \space | \space dD) \quad {Pr(dD | +D)} \\
        & + Pr(\RR \space | \space hD) \quad  {Pr(hD | +D)} \\  
        & = Pr(positive \space | \space dD) {^2} \quad  {0.009/0.01} \\
        & + Pr(positive  \space | \space hD) {^2} \quad  {0.001/0.01} \\
        & = 1.0^2 * 0.009/0.01 + 0.0^2 * 0.001/0.01  \\
        & = 0.9
\end{aligned}
$$
But if the two tests were independent conditionall on $+D$, we'd get



$$
\begin{aligned}
Pr(\RR \space | \space +D) 
        & = Pr(positive \space | \space +D) {^2} \\
        & = 0.9^2 \\
        & = 0.81
\end{aligned}
$$
## ## Show no cond indep for +D.

Nope.  
In the example, if conditionally independent, then  $Pr(\RR \space | \space +D)$  would equal $Pr(positive \space | \space +D) {^2}$ = $sensitivity^2 =0.82{^2} = 0.67$. 
But $Pr(\RR \space | \space +D) = 0.9^2 *0.9 + 0.1^2*0.1 = 0.73$, and that's  different, so NOT conditionally independent given $+D$.



## ## iD and noD = +D

On the no-disease side, 
if conditional independent, then  $Pr(positive \space | \space -D) {^2} = (1-specificity)^2 = 0.1{^2} = 0.01$. But $Pr(\RR \space | \space -D) = 1.0^2 *0.09 + 0.0^2*0.90 = 0.09$ is a lot bigger, so NOT AT ALL conditionally independent given $-D$.


## ## Two pictures



```{r}
makePosteriorSequence(priorFor4 = c(0.9, 0.1, 9, 90)/100, pr_pos_given_group = c(1,0,1,0))
```

$Pr(\RR \space | \space +D) = 1.00^2 *0.9 + 0.00^2*0.1 = 0.9$, but $Pr(positive \space | \space +D) {^2} = sensitivity^2 = 0.9{^2} = 0.81$. So NOT conditionally independent given $+D$.
Ideally you could carve up the population of patients into groups using explanations for why the groups differ. Then the only thing remaining would be pure chance mistakes, not explained by something about the particular patient. And so,  independence. 

That makes sense.  After the 1st report, the next bunch contribute no new information. $Pr(R2 = positive | R1 = positive) = 1$.
Any data whose probability equals one no matter what the truth is, is useless!   
And so we return to conspiracy theories and algorithms.


## ## Causal diagrams and colliders

## ## Machine learning

## ## Media bubbles

## ## Factoring joint distributions

## ## Intro to Gibbs sampling
